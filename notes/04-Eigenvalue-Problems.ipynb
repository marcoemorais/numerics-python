{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvalue Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.01 Eigenvalue Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an $n \\times n$ matrix $A$, find scalar eigenvalue $\\lambda$ and nonzero eigenvector $x$ such that:\n",
    "$$\n",
    "Ax = \\lambda x\n",
    "$$\n",
    "\n",
    "* Note: $\\lambda$ might be complex, even when $A$ is real\n",
    "* Interpretation: Eigenvalues and eigenvectors decompose complex linear behavior into simpler actions.\n",
    "\n",
    "**Spectrum** $\\lambda(A)$ is set of all eigenvalues of $A$\n",
    "\n",
    "**Spectral Radius** $\\rho(A)$ is maximum absolute value eigenvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use numpy to compute eigenvalues and eigenvectors of some simple matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Diagonal matrix.\n",
    "A = np.array([1,0,0,2]).reshape(2,2)\n",
    "w, v = np.linalg.eig(A)\n",
    "# Confirm that Av = wv.\n",
    "np.testing.assert_almost_equal(np.matmul(A, v), w * v)\n",
    "# Confirm eigenvalues are equal to diagonal.\n",
    "np.testing.assert_almost_equal(w, np.diag(A))\n",
    "\n",
    "# Triangular matrix.\n",
    "A = np.array([1,1,0,2]).reshape(2,2)\n",
    "w, v = np.linalg.eig(A)\n",
    "np.testing.assert_almost_equal(np.matmul(A, v), w * v)\n",
    "# Confirm eigenvalues are equal to diagonal.\n",
    "np.testing.assert_almost_equal(w, np.diag(A))\n",
    "\n",
    "# Symmetric matrix.\n",
    "A = np.array([3,-1,-1,3]).reshape(2,2)\n",
    "w, v = np.linalg.eig(A)\n",
    "np.testing.assert_almost_equal(np.matmul(A, v), w * v)\n",
    "\n",
    "# Symmetric matrix.\n",
    "A = np.array([1.5,0.5,0.5,1.5]).reshape(2,2)\n",
    "w, v = np.linalg.eig(A)\n",
    "np.testing.assert_almost_equal(np.matmul(A, v), w * v)\n",
    "\n",
    "# Complex matrix.\n",
    "A = np.array([0,1,-1,0]).reshape(2,2)\n",
    "w, v = np.linalg.eig(A)\n",
    "np.testing.assert_almost_equal(np.matmul(A, v), w * v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.02 Characteristic Polynomial and Multiplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvalue problem can be rewritten as $A$ with the eigenvalues $\\lambda$ subtracted from the diagonal.\n",
    "$$\n",
    "(A - \\lambda I) x = 0\n",
    "$$\n",
    "\n",
    "In order for the eigenvectors $x$ from the equation above to be nonsingular, then the matrix $(A - \\lambda I)$ must be singular. The determinant of $(A - \\lambda I)$ is known as the **characteristic equation** of the matrix $A$.\n",
    "$$\n",
    "\\text{det}(A - \\lambda I) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a - \\lambda & b \\\\\n",
    "c & d - \\lambda \\\\\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "= (a - \\lambda) (d - \\lambda) - bc \\\\\n",
    "= \\lambda^2 - (a + d) \\lambda + (ad - bc)\n",
    "$$\n",
    "where\n",
    "* $(a + d)$ is the trace of the matrix $A$\n",
    "* $(ad - bc)$ is the determinant of the matrix $A$\n",
    "\n",
    "In other words, the eigenvalues of $A$ are the roots of the **characteristic equation**.\n",
    "* Useful theoretical tool, but not useful numerically.\n",
    "\n",
    "#### Companion Matrix\n",
    "For any polynomial, there is an associated matrix whose eigenvalues are the roots of the polynomial.\n",
    "* The polynomial is referred to as **monic**.\n",
    "* The matrix is referred to as **companion matrix**.\n",
    "* The companion matrix has value 1 on the subdiagonal and coefficients of the polynomial in the last column of the matrix.\n",
    "\n",
    "#### Multiplicity\n",
    "Number of times root appears when polynomial is written as product of linear factors.\n",
    "* Defective matrix has eigenvalue of multiplicity $k \\gt 1$ with fewer than $k$ linearly independent corresponding eigenvectors.\n",
    "\n",
    "#### Diagonalizable\n",
    "Nondefective matrix $A$ with $n$ linearly independent eigenvectors has the similarity transform:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "AX &= XD \\\\\n",
    "X^{-1}AX &= D\n",
    "\\end{aligned}\n",
    "$$\n",
    "where\n",
    "* $D$ is a diagonal matrix with $\\lambda_1 \\cdots \\lambda_n$ along diagonal\n",
    "* $X$ is a matrix formed from the set of linearly independent eigenvectors $x_1 \\cdots x_n$\n",
    "\n",
    "#### Uniqueness\n",
    "Eigenvectors are not unique since they can be scaled arbitrarily.  As a result, eigenvectors are typically normalized to unit vector.\n",
    "\n",
    "Eigenspace, $S_{\\lambda}$, is the set of all eigenvectors such that $Ax = \\lambda x$.\n",
    "* $S_{\\lambda}$ is a subspace of $\\mathbb{R}^n$ or $\\mathbb{C}^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.03 Relevant Properties of Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properties Relevant to eigenvalue problems\n",
    "* diagonal\n",
    "* tridiagonal\n",
    "  * elements above the first superdiagonal and below the first subdiagonal are 0\n",
    "* triangular\n",
    "* Hessenberg\n",
    "  * like a triangular matrix, except either first superdiagonal or first subdiagonal are also nonzero\n",
    "* orthogonal\n",
    "  * $A^T A = A A^T = I$ where columns are orthonormal\n",
    "* unitary\n",
    "  * $A^H A = A A^H = I$ where $H$ is the conjugate transpose\n",
    "  * complex analogue to orthogonal\n",
    "* symmetric\n",
    "  * $A = A^T$\n",
    "* Hermitian\n",
    "  * $A = H^H$\n",
    "  * complex analogue to transpose\n",
    "* normal\n",
    "  * $A^H A = A A^H$ \n",
    "  * eigenvectors are orthogonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.04 Conditioning of Eigenvalue Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditioning refers to the sensitivity of the eigenvalues and eigenvectors to changes in matrix.\n",
    "* Previously we looked at conditioning as a property of a matrix, but for eigenvalue problems it is a property of the eigenvectors.\n",
    "\n",
    "For normal matrix $A^H A = A A^H$ eigenvectors are orthogonal and eigenvalues are well-conditioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check properties and conditioning of an example matrix with numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues:  [1. 2. 3.]\n",
      "is_normal:  False\n",
      "cond(v):  1288.943965937134\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([-149,-50,-154,537,180,546,-27,-9,-25]).reshape(3,3)\n",
    "\n",
    "# Compute the eigenvalues and right eigenvector.\n",
    "w, v = np.linalg.eig(A)\n",
    "print(\"eigenvalues: \", w)\n",
    "\n",
    "# Since eigenvalues are distinct, A is diagonalizable.\n",
    "# A X = X D where D is the diagonal matrix formed from eigenvalues.\n",
    "np.testing.assert_almost_equal(np.matmul(A, v), np.matmul(v, np.diag(w)))\n",
    "\n",
    "# Check whether A is normal.\n",
    "is_normal = np.equal(np.matmul(A.T, A), np.matmul(A, A.T))\n",
    "print(\"is_normal: \", np.all(is_normal))\n",
    "\n",
    "# Check the condition number of the right eigenvector.\n",
    "# If value is large, then eigenvalues are sensitive to perturbations in A.\n",
    "print(\"cond(v): \", np.linalg.cond(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.05 Computing Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Transformations\n",
    "* Shift\n",
    "  * If $Ax = \\lambda x$ and $\\sigma$ is a scalar, then $(A - \\sigma I) x = (\\lambda - \\sigma) x$.\n",
    "  * Eigenvalues of shifted matrix are shifted eigenvalues of original matrix.\n",
    "* Inversion\n",
    "  * If $A$ is nonsingular and $Ax = \\lambda x$ with $x \\neq 0$, then $A^{-1}x = \\frac{1}{\\lambda}x$.\n",
    "* Powers\n",
    "  * If $Ax = \\lambda x$, then $A^k x = \\lambda^k x$\n",
    "* Polynomial\n",
    "  * If $Ax = \\lambda x$ and $p(t)$ is a polynomial, then $p(A)x = p(\\lambda)x$\n",
    "* Similarity\n",
    "  * $B$ is similar to $A$ if there is a nonsingular matrix $T$ such that $B = T^{-1} A T$.\n",
    "  * $A$ and $B$ will have the same eigenvalues.\n",
    "  * If $y$ is an eigenvector of $B$, then $x = Ty$ is an eigenvector of $A$.\n",
    "\n",
    "Why are similarity transforms useful?\n",
    "* $X^{-1} A X = D$ yields diagonal matrix $D$\n",
    "* $D$ has eigenvalues along diagonal\n",
    "* $X$ has eigenvectors as columns of an identity matrix\n",
    "* Not all matrices are diagonalizable by similarity transformation.\n",
    "\n",
    "#### Forms Attainable By Similarity\n",
    "$B$ is similar to $A$ if there is a nonsingular matrix $T$ such that $B = T^{-1} A T$.\n",
    "\n",
    "| A | T | B |\n",
    "|---|---|---|\n",
    "| distinct eigenvalues | nonsingular | diagonal |\n",
    "| real symmetric | orthogonal | real diagonal |\n",
    "| complex Hermitian | unitary | real diagonal |\n",
    "| normal | unitary | diagonal |\n",
    "| arbitrary real | orthogonal | real block triangular |\n",
    "| arbitrary | unitary | upper triangular |\n",
    "| arbitrary | nonsingular | almost diagonal (Jordan) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.06 Power Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the dominant eigenvalue $\\lambda_k$ and eigenvector $x_k$ of the $n \\times n$ matrix $A$.\n",
    "1. Start with some nonzero vector $x_0$.\n",
    "2. Compute $x_k = A x_{k-1}$ and $\\lambda_k = x_k^T \\cdot A x_k$.\n",
    "  * To avoid overflow (or underflow if $\\lambda_k \\lt 1$), consider normalizing $x_k$ after each iteration.\n",
    "3. Repeat the previous step until $| \\lambda_k - \\lambda_{k-1} | \\lt \\epsilon$.\n",
    "\n",
    "After convergence, the value $\\lambda_k$ is the dominant eigenvalue with  eigenvector $x_k$.\n",
    "* The dominant eigenvalue is eigenvalue having maximum absolute value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the dominant eigenvalue and eigenvector of the $n \\times n$ matrix $A$ using power iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def power_iteration(A):\n",
    "    \"\"\"\n",
    "    Use power iteration to compute the dominant eigenvalue of A.\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    x_k = np.random.random((n,))  # Any nonzero vector.\n",
    "    lambda_k, lambda_prev = 1., 0. \n",
    "\n",
    "    while abs(lambda_k - lambda_prev) > np.finfo('d').eps:\n",
    "        x_k = np.matmul(A, x_k)\n",
    "        x_k = x_k / np.linalg.norm(x_k)\n",
    "\n",
    "        lambda_prev = lambda_k\n",
    "        lambda_k = np.dot(x_k.T, np.matmul(A, x_k))\n",
    "\n",
    "    return lambda_k, x_k\n",
    "\n",
    "A = np.array([3,1,1,3]).reshape(2,2)\n",
    "\n",
    "# Compute the dominant eigenvalue and corresponding eigenvector.\n",
    "w_max, v_max = power_iteration(A)\n",
    "\n",
    "# Compute the eigenvalues and right eigenvector using numpy.\n",
    "w, v = np.linalg.eig(A)\n",
    "\n",
    "# Compare the dominant eigenvalue to numpy.\n",
    "np.testing.assert_almost_equal(w_max, np.max(np.absolute(w)),)\n",
    "\n",
    "# Compare the dominant eigenvector to numpy.\n",
    "np.testing.assert_almost_equal(v_max, v[:, np.argmax(w)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.07 Inverse and Rayleigh Quotient Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the smallest eigenvalue of matrix $A$, make use of eigenvalues of $A^{-1}$ are reciprocals of $A$.  As a result, smallest eigenvalue of $A$ is the reciprocal of largest eigenvalue of $A^{-1}$.\n",
    "* In practice, we factorize $A$ and solve for $x_k$ rather than explicitly computing $A^{-1}$.\n",
    "\n",
    "#### Inverse Iteration\n",
    "Compute the smallest eigenvalue $\\lambda_k$ and eigenvector $x_k$ of the $n \\times n$ matrix $A$.\n",
    "1. Start with some nonzero vector $x_0$.\n",
    "2. Solve $A y_k = x_{k-1}$ for $y_k$.\n",
    "2. Compute $x_k = y_k / ||y_k||_{\\inf}$ and $\\lambda_k = x_k^T \\cdot A x_k$.\n",
    "3. Repeat the previous step until $| \\lambda_k - \\lambda_{k-1} | \\lt \\epsilon$.\n",
    "\n",
    "#### Rayleigh Quotient\n",
    "Given an approximate eigenvector $x$ for real matrix $A$ find best estimate for eigenvalue $\\lambda$ by solving $x \\lambda \\approxeq A x$ for $\\lambda$ using **Rayleigh Quotient**:\n",
    "$$\n",
    "\\lambda = \\frac{x^T A x}{x^T x}\n",
    "$$\n",
    "\n",
    "The Rayleigh Quotient can be be combined with the inverse iteration method by solving the shifted matrix $(A - \\lambda_k I) y_k = x_{k-1}$ for $y_k$.\n",
    "* Since the shifted matrix changes with each iteration, the factorization must be repeated each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the smallest eigenvalue and eigenvector of the $n \\times n$ matrix $A$ using inverse iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "\n",
    "def inverse_iteration(A):\n",
    "    \"\"\"\n",
    "    Use inverse iteration to compute the smallest eigenvalue of A.\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    x_k = np.random.random((n,))  # Any nonzero vector.\n",
    "    lambda_k, lambda_prev = 1., 0. \n",
    "    \n",
    "    # Factorize A into [L|U].\n",
    "    # NOTE(mmorais): Use P^T to reorder the right-hand side.\n",
    "    P, L, U = la.lu(A)\n",
    "\n",
    "    while abs(lambda_k - lambda_prev) > np.finfo('d').eps:\n",
    "        # Solve A y_k = x_k for y_k.\n",
    "        # 1. Solve L yy = x_k for yy.\n",
    "        yy = la.solve_triangular(L, np.matmul(P.T, x_k), lower=True)\n",
    "        # 2. Solve U y_k = yy for y_k.\n",
    "        y_k = la.solve_triangular(U, yy, lower=False)\n",
    "        x_k = y_k / np.linalg.norm(y_k)\n",
    "\n",
    "        lambda_prev = lambda_k\n",
    "        lambda_k = np.dot(x_k.T, np.matmul(A, x_k))\n",
    "\n",
    "    return lambda_k, x_k\n",
    "\n",
    "A = np.array([3,1,1,3]).reshape(2,2)\n",
    "\n",
    "# Compute the smallest eigenvalue and corresponding eigenvector.\n",
    "w_min, v_min = inverse_iteration(A)\n",
    "\n",
    "# Compute the eigenvalues and right eigenvector using numpy.\n",
    "w, v = np.linalg.eig(A)\n",
    "\n",
    "# Compare the smallest eigenvalue to numpy.\n",
    "np.testing.assert_almost_equal(w_min, np.min(np.absolute(w)),)\n",
    "\n",
    "# Compare the smallest eigenvector to numpy.\n",
    "np.testing.assert_almost_equal(v_min, v[:, np.argmin(w)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.08 Deflation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.09 QR Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.10 Krylov Subspace Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.11 Jacobi Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.12 Other Methods for Symmetric Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.13 Generalized Eigenvalue Problems and SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
