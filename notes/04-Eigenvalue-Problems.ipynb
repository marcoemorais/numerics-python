{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvalue Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.01 Eigenvalue Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an $n \\times n$ matrix $A$, find scalar eigenvalue $\\lambda$ and nonzero eigenvector $x$ such that:\n",
    "$$\n",
    "Ax = \\lambda x\n",
    "$$\n",
    "\n",
    "* Note: $\\lambda$ might be complex, even when $A$ is real\n",
    "* $x$ is sometimes referred to as the *right eigenvector*\n",
    "* Interpretation: Eigenvalues and eigenvectors decompose complex linear behavior into simpler actions.\n",
    "\n",
    "Alternatively we have left eigenvector\n",
    "$$\n",
    "y^T A = \\lambda y^T\n",
    "$$\n",
    "where\n",
    "* $y$ is the left eigenvector\n",
    "* Theoretically interesting, but not important for computation\n",
    "\n",
    "**Spectrum** $\\lambda(A)$ is set of all eigenvalues of $A$\n",
    "\n",
    "**Spectral Radius** $\\rho(A)$ is maximum absolute value eigenvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use numpy to compute eigenvalues and eigenvectors of some simple matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Diagonal matrix.\n",
    "A = np.array([1,0,0,2]).reshape(2,2)\n",
    "w, v = np.linalg.eig(A)\n",
    "# Confirm that Av = wv.\n",
    "np.testing.assert_almost_equal(np.matmul(A, v), w * v)\n",
    "# Confirm eigenvalues are equal to diagonal.\n",
    "np.testing.assert_almost_equal(w, np.diag(A))\n",
    "\n",
    "# Triangular matrix.\n",
    "A = np.array([1,1,0,2]).reshape(2,2)\n",
    "w, v = np.linalg.eig(A)\n",
    "np.testing.assert_almost_equal(np.matmul(A, v), w * v)\n",
    "# Confirm eigenvalues are equal to diagonal.\n",
    "np.testing.assert_almost_equal(w, np.diag(A))\n",
    "\n",
    "# Symmetric matrix.\n",
    "A = np.array([3,-1,-1,3]).reshape(2,2)\n",
    "w, v = np.linalg.eig(A)\n",
    "np.testing.assert_almost_equal(np.matmul(A, v), w * v)\n",
    "\n",
    "# Symmetric matrix.\n",
    "A = np.array([1.5,0.5,0.5,1.5]).reshape(2,2)\n",
    "w, v = np.linalg.eig(A)\n",
    "np.testing.assert_almost_equal(np.matmul(A, v), w * v)\n",
    "\n",
    "# Complex matrix.\n",
    "A = np.array([0,1,-1,0]).reshape(2,2)\n",
    "w, v = np.linalg.eig(A)\n",
    "np.testing.assert_almost_equal(np.matmul(A, v), w * v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.02 Characteristic Polynomial and Multiplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvalue problem can be rewritten as $A$ with the eigenvalues $\\lambda$ subtracted from the diagonal.\n",
    "$$\n",
    "(A - \\lambda I) x = 0\n",
    "$$\n",
    "\n",
    "In order for the eigenvectors $x$ from the equation above to be nonsingular, then the matrix $(A - \\lambda I)$ must be singular. The determinant of $(A - \\lambda I)$ is known as the **characteristic equation** of the matrix $A$.\n",
    "$$\n",
    "\\text{det}(A - \\lambda I) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a - \\lambda & b \\\\\n",
    "c & d - \\lambda \\\\\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "= (a - \\lambda) (d - \\lambda) - bc \\\\\n",
    "= \\lambda^2 - (a + d) \\lambda + (ad - bc)\n",
    "$$\n",
    "where\n",
    "* $(a + d)$ is the trace of the matrix $A$\n",
    "* $(ad - bc)$ is the determinant of the matrix $A$\n",
    "\n",
    "In other words, the eigenvalues of $A$ are the roots of the **characteristic equation**.\n",
    "* Useful theoretical tool, but not useful numerically.\n",
    "\n",
    "#### Companion Matrix\n",
    "For any polynomial, there is an associated matrix whose eigenvalues are the roots of the polynomial.\n",
    "* The polynomial is referred to as **monic**.\n",
    "* The matrix is referred to as **companion matrix**.\n",
    "* The companion matrix has value 1 on the subdiagonal and coefficients of the polynomial in the last column of the matrix.\n",
    "\n",
    "#### Multiplicity\n",
    "Number of times root appears when polynomial is written as product of linear factors.\n",
    "* Defective matrix has eigenvalue of multiplicity $k \\gt 1$ with fewer than $k$ linearly independent corresponding eigenvectors.\n",
    "\n",
    "#### Diagonalizable\n",
    "Nondefective matrix $A$ with $n$ linearly independent eigenvectors has the similarity transform:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "AX &= XD \\\\\n",
    "X^{-1}AX &= D\n",
    "\\end{aligned}\n",
    "$$\n",
    "where\n",
    "* $D$ is a diagonal matrix with $\\lambda_1 \\cdots \\lambda_n$ along diagonal\n",
    "* $X$ is a matrix formed from the set of linearly independent eigenvectors $x_1 \\cdots x_n$\n",
    "\n",
    "#### Uniqueness\n",
    "Eigenvectors are not unique since they can be scaled arbitrarily.  \n",
    "* Eigenvectors are typically normalized to unit vector.\n",
    "* They can be scaled arbitrarily (eg multiplied by 1 or -1) to flip sign of components.\n",
    "\n",
    "Eigenspace, $S_{\\lambda}$, is the set of all eigenvectors such that $Ax = \\lambda x$.\n",
    "* $S_{\\lambda}$ is a subspace of $\\mathbb{R}^n$ or $\\mathbb{C}^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.03 Relevant Properties of Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properties Relevant to eigenvalue problems\n",
    "* diagonal\n",
    "  * eigenvalues are diagonal entries\n",
    "* triangular\n",
    "  * eigenvalues are diagonal entries\n",
    "* tridiagonal\n",
    "  * elements above the first superdiagonal and below the first subdiagonal are 0\n",
    "* Hessenberg\n",
    "  * like a triangular matrix, except either first superdiagonal or first subdiagonal are also nonzero\n",
    "* orthogonal\n",
    "  * $A^T A = A A^T = I$ where columns are orthonormal\n",
    "* unitary\n",
    "  * $A^H A = A A^H = I$ where $H$ is the conjugate transpose\n",
    "  * complex analogue to orthogonal\n",
    "* symmetric\n",
    "  * $A = A^T$\n",
    "  * eigenvalues must be real\n",
    "* Hermitian\n",
    "  * $A = H^H$\n",
    "  * complex analogue to transpose\n",
    "  * eigenvalues must be real\n",
    "* normal\n",
    "  * $A^H A = A A^H$ \n",
    "  * eigenvectors are orthogonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.04 Conditioning of Eigenvalue Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditioning refers to the sensitivity of the eigenvalues and eigenvectors to changes in matrix.\n",
    "* Previously we looked at conditioning as a property of a matrix, but for eigenvalue problems it is a property of the eigenvectors.\n",
    "\n",
    "For normal matrix $A^H A = A A^H$ eigenvectors are orthogonal and eigenvalues are well-conditioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check properties and conditioning of an example matrix with numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues:  [1. 2. 3.]\n",
      "is_normal:  False\n",
      "cond(v):  1288.943965937134\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([-149,-50,-154,537,180,546,-27,-9,-25]).reshape(3,3)\n",
    "\n",
    "# Compute the eigenvalues and right eigenvector.\n",
    "w, v = np.linalg.eig(A)\n",
    "print(\"eigenvalues: \", w)\n",
    "\n",
    "# Since eigenvalues are distinct, A is diagonalizable.\n",
    "# A X = X D where D is the diagonal matrix formed from eigenvalues.\n",
    "np.testing.assert_almost_equal(np.matmul(A, v), np.matmul(v, np.diag(w)))\n",
    "\n",
    "# Check whether A is normal.\n",
    "is_normal = np.equal(np.matmul(A.T, A), np.matmul(A, A.T))\n",
    "print(\"is_normal: \", np.all(is_normal))\n",
    "\n",
    "# Check the condition number of the right eigenvector.\n",
    "# If value is large, then eigenvalues are sensitive to perturbations in A.\n",
    "print(\"cond(v): \", np.linalg.cond(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.05 Computing Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Transformations\n",
    "* Shift\n",
    "  * If $Ax = \\lambda x$ and $\\sigma$ is a scalar, then $(A - \\sigma I) x = (\\lambda - \\sigma) x$.\n",
    "  * Eigenvalues of shifted matrix are shifted eigenvalues of original matrix.\n",
    "  * Eigenvectors are unchanged.\n",
    "* Inversion\n",
    "  * If $A$ is nonsingular and $Ax = \\lambda x$ with $x \\neq 0$, then $A^{-1}x = \\frac{1}{\\lambda}x$.\n",
    "  * Eigenvectors are unchanged.\n",
    "* Powers\n",
    "  * If $Ax = \\lambda x$, then $A^k x = \\lambda^k x$\n",
    "  * Raising a matrix to a power $k$ also raises eigenvalues to power $k$.\n",
    "  * Eigenvectors are unchanged.\n",
    "* Polynomial\n",
    "  * If $Ax = \\lambda x$ and $p(t)$ is a polynomial, then $p(A)x = p(\\lambda)x$\n",
    "* Similarity\n",
    "  * $B$ is similar to $A$ if there is a nonsingular matrix $T$ such that $B = T^{-1} A T$.\n",
    "  * $A$ and $B$ will have the same eigenvalues.\n",
    "  * If $y$ is an eigenvector of $B$, then $x = Ty$ is an eigenvector of $A$.\n",
    "\n",
    "Why are similarity transforms useful?\n",
    "* $X^{-1} A X = D$ yields diagonal matrix $D$\n",
    "* $D$ has eigenvalues along diagonal\n",
    "* $X$ has eigenvectors as columns of an identity matrix\n",
    "* Not all matrices are diagonalizable by similarity transformation.\n",
    "\n",
    "#### Forms Attainable By Similarity\n",
    "$B$ is similar to $A$ if there is a nonsingular matrix $T$ such that $B = T^{-1} A T$.\n",
    "\n",
    "| A | T | B |\n",
    "|---|---|---|\n",
    "| distinct eigenvalues | nonsingular | diagonal |\n",
    "| real symmetric | orthogonal | real diagonal |\n",
    "| complex Hermitian | unitary | real diagonal |\n",
    "| normal | unitary | diagonal |\n",
    "| arbitrary real | orthogonal | real block triangular |\n",
    "| arbitrary | unitary | upper triangular |\n",
    "| arbitrary | nonsingular | almost diagonal (Jordan) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.06 Power Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the dominant eigenvalue $\\lambda_k$ and eigenvector $x_k$ of the $n \\times n$ matrix $A$.\n",
    "1. Start with some nonzero vector $x_0$.\n",
    "2. Compute $x_k = A x_{k-1}$ and $\\lambda_k = x_k^T \\cdot A x_k$.\n",
    "  * To avoid overflow (or underflow if $\\lambda_k \\lt 1$), consider normalizing $x_k$ after each iteration.\n",
    "3. Repeat the previous step until $| \\lambda_k - \\lambda_{k-1} | \\lt \\epsilon$.\n",
    "\n",
    "After convergence, the value $\\lambda_k$ is the dominant eigenvalue with  eigenvector $x_k$.\n",
    "* The dominant eigenvalue is eigenvalue having maximum absolute value.\n",
    "\n",
    "The convergence rate of power iteration depends on the ratio $|\\lambda_2 / \\lambda_1|$ where $|\\lambda_2|$ is the eigenvalue having second-largest absolute value.\n",
    "* Smaller the ratio, the faster the convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the dominant eigenvalue and eigenvector of the $n \\times n$ matrix $A$ using power iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def power_iteration(A):\n",
    "    \"\"\"\n",
    "    Use power iteration to compute the dominant eigenvalue of A.\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    x_k = np.random.random((n,))  # Any nonzero vector.\n",
    "    lambda_k, lambda_prev = 1., 0. \n",
    "\n",
    "    while abs(lambda_k - lambda_prev) > np.finfo('d').eps:\n",
    "        x_k = np.matmul(A, x_k)\n",
    "        x_k = x_k / np.linalg.norm(x_k)\n",
    "\n",
    "        # Use lambda as the termination condition.\n",
    "        lambda_prev = lambda_k\n",
    "        lambda_k = np.dot(x_k.T, np.matmul(A, x_k))\n",
    "\n",
    "    return lambda_k, x_k\n",
    "\n",
    "A = np.array([3,1,1,3]).reshape(2,2)\n",
    "\n",
    "# Compute the dominant eigenvalue and corresponding eigenvector.\n",
    "w_max, v_max = power_iteration(A)\n",
    "\n",
    "# Compute the eigenvalues and right eigenvector using numpy.\n",
    "w, v = np.linalg.eig(A)\n",
    "\n",
    "# Compare the dominant eigenvalue to numpy.\n",
    "np.testing.assert_almost_equal(w_max, np.max(np.absolute(w)),)\n",
    "\n",
    "# Compare the dominant eigenvector to numpy.\n",
    "np.testing.assert_almost_equal(v_max, v[:, np.argmax(w)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.07 Inverse and Rayleigh Quotient Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the smallest eigenvalue of matrix $A$, make use of eigenvalues of $A^{-1}$ are reciprocals of $A$.  As a result, smallest eigenvalue of $A$ is the reciprocal of largest eigenvalue of $A^{-1}$.\n",
    "* In practice, we factorize $A$ and solve for $x_k$ rather than explicitly computing $A^{-1}$.\n",
    "\n",
    "#### Inverse Iteration\n",
    "Compute the smallest eigenvalue $\\lambda_k$ and eigenvector $x_k$ of the $n \\times n$ matrix $A$.\n",
    "1. Start with some nonzero vector $x_0$.\n",
    "2. Solve $A y_k = x_{k-1}$ for $y_k$.\n",
    "2. Compute $x_k = y_k / ||y_k||_{\\inf}$ and $\\lambda_k = x_k^T \\cdot A x_k$.\n",
    "3. Repeat the previous step until $| \\lambda_k - \\lambda_{k-1} | \\lt \\epsilon$.\n",
    "\n",
    "#### Rayleigh Quotient\n",
    "Given an approximate eigenvector $x$ for real matrix $A$ find best estimate for eigenvalue $\\lambda$ by solving $x \\lambda \\approxeq A x$ for $\\lambda$ using **Rayleigh Quotient**:\n",
    "$$\n",
    "\\lambda = \\frac{x^T A x}{x^T x}\n",
    "$$\n",
    "\n",
    "The Rayleigh Quotient can be be combined with the inverse iteration method by solving the shifted matrix $(A - \\lambda_k I) y_k = x_{k-1}$ for $y_k$.\n",
    "* Since the shifted matrix changes with each iteration, the factorization must be repeated each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the smallest eigenvalue and eigenvector of the $n \\times n$ matrix $A$ using inverse iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "\n",
    "def inverse_iteration(A):\n",
    "    \"\"\"\n",
    "    Use inverse iteration to compute the smallest eigenvalue of A.\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    x_k = np.random.random((n,))  # Any nonzero vector.\n",
    "    lambda_k, lambda_prev = 1., 0. \n",
    "\n",
    "    # Factorize A into [L|U].\n",
    "    # NOTE(mmorais): Use P^T to reorder the right-hand side.\n",
    "    P, L, U = la.lu(A)\n",
    "\n",
    "    while abs(lambda_k - lambda_prev) > np.finfo('d').eps:\n",
    "        # Solve A y_k = x_k for y_k.\n",
    "        # 1. Solve L yy = x_k for yy.\n",
    "        yy = la.solve_triangular(L, np.matmul(P.T, x_k), lower=True)\n",
    "        # 2. Solve U y_k = yy for y_k.\n",
    "        y_k = la.solve_triangular(U, yy, lower=False)\n",
    "        x_k = y_k / np.linalg.norm(y_k)\n",
    "\n",
    "        # Use lambda as the termination condition.\n",
    "        lambda_prev = lambda_k\n",
    "        lambda_k = np.dot(x_k.T, np.matmul(A, x_k))\n",
    "\n",
    "    return lambda_k, x_k\n",
    "\n",
    "A = np.array([3,1,1,3]).reshape(2,2)\n",
    "\n",
    "# Compute the smallest eigenvalue and corresponding eigenvector.\n",
    "w_min, v_min = inverse_iteration(A)\n",
    "\n",
    "# Compute the eigenvalues and right eigenvector using numpy.\n",
    "w, v = np.linalg.eig(A)\n",
    "\n",
    "# Compare the smallest eigenvalue to numpy.\n",
    "np.testing.assert_almost_equal(w_min, np.min(np.absolute(w)),)\n",
    "\n",
    "# Compare the smallest eigenvector to numpy.\n",
    "# NOTE(mmorais): Use absolute value of components of eigenvector.\n",
    "# Since eigenvectors are not unique, they can be scaled arbitrarily\n",
    "# (eg multiplied by 1 or -1).\n",
    "np.testing.assert_almost_equal(np.abs(v_min), np.abs(v[:, np.argmin(w)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.08 Deflation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power iteration and inverse iteration can be used to find the maximum and minimum eigenvalues respectively, but another technique is needed to find all the eigenvalues of a matrix.\n",
    "\n",
    "**Deflation** is the process of removing known eigenvalues in order to compute the remaining eigenvalues of a matrix.\n",
    "\n",
    "Let $H$ be a nonsingular matrix such that $H x_1 = \\alpha e_1$ where $x_1$ is the first eigenvector of the matrix and $e_1$ is the elementary row matrix.  Then similarity transformation below can be used to transform the $n \\times n$ matrix $A$ to the $n-1 \\times n-1$ matrix $B$ having eigenvalues $\\lambda_2, \\cdots, \\lambda_n$.\n",
    "$$\n",
    "H A H^{-1} = \n",
    "\\begin{bmatrix}\n",
    "\\lambda_1 & b^T \\\\\n",
    "0 & B\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "After finding the eigenvalue $\\lambda_2$ and eigenvector $y_2$ of $B$, then the following can be used to obtain the eigenvector $x_2$ of the original matrix $A$.\n",
    "$$\n",
    "x_2 = H^{-1}\n",
    "\\begin{bmatrix}\n",
    "\\alpha \\\\\n",
    "y_2\n",
    "\\end{bmatrix},\n",
    "\\qquad\n",
    "\\alpha = \\frac{b^T y_2}{\\lambda_2 - \\lambda_1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.09 QR Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simultaneous Iteration\n",
    "Rather than finding eigenvalue and eigenvector pairs one-by-one, the technique of simultaneous iteration finds all pairs starting from a **matrix** of initial vectors.\n",
    "$$\n",
    "X_k = A X_{k-1}\n",
    "$$\n",
    "where\n",
    "* $X_k$ is a matrix of eigenvectors, initialized with random values\n",
    "\n",
    "#### Orthogonal Iteration\n",
    "Similar to power iteration, normalization is required.  Use the QR factorization of $X_{k-1}$ at each step of the iteration.\n",
    "$$\n",
    "\\hat{Q}_k R_k = X_{k-1} \\\\\n",
    "X_k = A \\hat{Q}_k\n",
    "$$\n",
    "\n",
    "Why normalize?\n",
    "1. Avoid overflow and underflow.\n",
    "2. Columns of $X_k$ increasingly ill-conditioned, orthonormalization helps improve conditioning. \n",
    "\n",
    "#### QR Iteration\n",
    "Rather than explicitly computing the eigenvectors, use QR factorization to make $A_k$ converge to triangular form from which the eigenvalues can be extracted from the diagonals of $A_k$.\n",
    "1. Initialize $A_0$ to $A$.\n",
    "2. Compute QR factorization $Q_k R_k$ of $A_{k-1}$.\n",
    "3. Update $A_k = R_k Q_k$\n",
    "4. Repeat starting from step 2.\n",
    "\n",
    "The eigenvectors of $A$ are obtained from product of $Q_k$ matrices generated during iteration.\n",
    "\n",
    "#### QR Iteration With Shifts\n",
    "Shift the matrix $A_{k-1}$ by $\\sigma_k$ prior to computing the QR factorization.\n",
    "1. Initialize $A_0$ to $A$.\n",
    "2. Compute QR factorization $Q_k R_k$ of $A_{k-1} - \\sigma_k I$.\n",
    "3. Update $A_k = R_k Q_k + \\sigma_k I$\n",
    "4. Repeat starting from step 2.\n",
    "\n",
    "The choice of shift $\\sigma_k$ need only approximate an eigenvalue.\n",
    "* The lower right corner of $A_k$ is a good approximation. \n",
    "\n",
    "**Initial Reduction** Transform $A$ to Hessenberg matrix prior to QR iteration using orthogonal similarity transformation such as Householder transformation.\n",
    "  * Since Hessenberg is nearly triangular, work per QR iteration is reduced and fewer iterations are required since matrix is already nearly triangular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost of QR Iteration\n",
    "\n",
    "| Matrix | Eigenvalues Only | Eigenvalues & Eigenvectors |\n",
    "|--------|------------------|----------------------------|\n",
    "| Symmetric | $\\frac{4}{3} n^3$ | $9 n^3$ |\n",
    "| General | $10 n^3$ | $25 n^3$ |\n",
    "\n",
    "Interpretation: Cost of obtaining eigenvectors by accumulating orthogonal transformations is high. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.10 Krylov Subspace Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike simultaneous methods such as QR iteration, Krylov methods build up the subspace incrementally.\n",
    "* Requires only matrix-vector multiplication.\n",
    "* Useful for large sparse matrices.\n",
    "\n",
    "**Arnoldi iteration** recurrence used to produce unitary matrix $Q_k$ and upper Hessenberg matrix $H_k$ column-by-column using only matrix-vector multiplication.\n",
    "* The matrix $H_k$ only provides an approximation to eigenvalues and eigenvectors of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.11 Jacobi Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jacobi method used to compute eigenvalues of real symmetric matrix.\n",
    "$$\n",
    "A_{k+1} = J_k^T A_k J_k\n",
    "$$\n",
    "where\n",
    "* $J_k$ is a plane rotation used to annihilate pairs of symmetric entries in $A_k$\n",
    "\n",
    "Eventually $A$ convergences to diagonal form.\n",
    "* Eigenvalues are diagonal entries.\n",
    "* Eigenvectors obtained from the product of plane rotations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.12 Other Methods for Symmetric Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bispectrum** or **Spectrum Slicing** used to determine how many eigenvalues are less than $\\sigma$.\n",
    "* By systemmatically choosing values of $\\sigma$ any eigenvalue can be isolated using bisection technique.\n",
    "\n",
    "**Divide and Conquer** used to find eigenvalues and eigenvectors of symmetric tridiagonal matrices.\n",
    "* Recursive algorithm which expresses the original matrix as sum of two matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04.13 Generalized Eigenvalue Problems and SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalized Eigenvalue Problem\n",
    "Find the eigenvalues and eigenvectors of $A$ and $B$ which are $n \\times n$ matrices. \n",
    "$$\n",
    "A x = \\lambda B x\n",
    "$$\n",
    "\n",
    "Application to structural vibration problems:\n",
    "* $A$ represents stiffness\n",
    "* $B$ represents mass\n",
    "\n",
    "#### SVD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Eigenvalue Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Algorithms for computing eigenvalues and eigenvectors are iterative.\n",
    "* Power and inverse iteration find one eigenvalue-eigenvector pair.\n",
    "* QR iteration simultaneously finds all eigenvalue and eigenvectors by transforming matrix to triangular form using orthogonal similarity.\n",
    "  * An initial reduction of the matrix to Hessenberg form is often used in practice in order to improve efficiency.\n",
    "* Krylov methods are used for large sparse matrices.\n",
    "* More specialized methods exist for symmetric matrices.\n",
    "  * Jacobi\n",
    "  * Spectrum Slicing\n",
    "  * Divide and Conquer\n",
    "* SVD can be computed by a variant of QR iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
