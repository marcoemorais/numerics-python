{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05.01 Nonlinear Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a function f, we seek a value $x$, known as **root** or **zero** of f, for which\n",
    "$$\n",
    "f(x) = 0\n",
    "$$\n",
    "\n",
    "Two cases.\n",
    "1. Single nonlinear equation in one unknown, $x$ is a scalar $f: \\mathbb{R} \\rightarrow \\mathbb{R}$\n",
    "\n",
    "Example:\n",
    "$$\n",
    "x^2 - 4 \\sin(x) = 0\n",
    "$$\n",
    "\n",
    "2. System of $n$ coupled nonlinear equations in $n$ unknowns, $x$ is vector\n",
    "$f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$\n",
    "\n",
    "Example:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_1^2 - x_2 + 0.25 &= 0 \\\\\n",
    "-x_1 + x_2^2 + 0.25 &= 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note: Finding $f(x) = 0$ for overdetermined systems, $m \\gt n$, is covered by the chapter on optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05.02 Existence, Uniqueness, and Conditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bracket** is interval \\[a, b\\] for which sign of $f$ differs at $f(a)$ and $f(b)$.\n",
    "* *Intermediate Value Theorem* if $f$ is continuous and sign($f(a)$) $\\neq$ sign($f(b)$), then there exists some $x$ such that f(x) = 0.\n",
    "\n",
    "#### Conditioning\n",
    "Basic premise: If the function $f$ is insensitive to the value of the argument $x$, then the root will be sensitive.\n",
    "\n",
    "Two cases.\n",
    "1. $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ with solution $x^*$\n",
    "$$\n",
    "\\text{cond}(f) = 1 / |f'(x^*)|\n",
    "$$\n",
    "  * Root is ill-conditioned if $f'(x^*) = 0$ eg horizontal.\n",
    "\n",
    "2. $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ with solution $x^*$\n",
    "$$\n",
    "\\text{cond}(f) = ||J_f^{-1}(x^*)|| = \\partial{f_i(x)} / \\partial{x_j}\n",
    "$$\n",
    "  * The matrix $J_f$ is known as **Jacobian** aka matrix of partial derivatives.\n",
    "  * Root is ill-conditioned if $J_f$ is singular.\n",
    "\n",
    "#### Residual\n",
    "Useful as measure of error when root is not ill-conditioned.\n",
    "\n",
    "A small residual does not necessarily imply solution is accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05.03 Convergence of Iterative Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative Methods\n",
    "**Error** at iteration $k$ is given by $e_k$:\n",
    "$$\n",
    "e_k = x_k - x^*\n",
    "$$\n",
    "where\n",
    "* $x_k$ is approximate solution at iteration $k$\n",
    "  * For methods which maintain an interval rather than single point, then $x_k$ is the length of the interval and $x^*$ is zero.\n",
    "* $x^*$ is true solution\n",
    "\n",
    "**Convergence** given by ratio of errors of successive iterates and some rate $r$:\n",
    "$$\n",
    "\\lim\\limits_{k->\\inf} \\frac{||e_{k+1}||}{||e_k||^r} = C\n",
    "$$\n",
    "where\n",
    "* $C$ is a nonzero constant describing the change from one iteration to the next\n",
    "\n",
    "|  convergence rate $r$ | description | accuracy gained per iteration |\n",
    "|-----------------------|-------------|-------------------------------|\n",
    "| $r = 1$ | linear | constant |\n",
    "| $1 \\lt r \\lt 2$ | superlinear | increasing |\n",
    "| $r = 2$ | quadratic | double |\n",
    "\n",
    "**Stopping Criteria** describe conditions for terminating iteration. Typical choices include:\n",
    "1. Relative change in successive iterates is small.\n",
    "$$\n",
    "||x_{k+1} - x_k|| / ||x_k|| \\lt \\epsilon\n",
    "$$\n",
    "2. Residual is small.\n",
    "$$\n",
    "||f(x_k)|| \\lt \\epsilon\n",
    "$$\n",
    "\n",
    "These quantities are not necessarily small simultaneously depending on the problem conditioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05.04 Bisection Method in 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bisection method begins with bracket given by $[a, b]$, repeatedly halving the interval until solution is found to some particular accuracy.\n",
    "1. Compute the value of $f$ at midpoint $m$ of the bracket $[a, b]$.\n",
    "2. If sign(f(a)) $\\neq$ sign(f(m)), then $b = m$, else $a = m$.\n",
    "3. Repeat while $b - a \\gt \\epsilon$.\n",
    "\n",
    "Pros\n",
    "* Certain to converge\n",
    "* Simple, only makes use of sign of function, not magnitude\n",
    "\n",
    "Cons\n",
    "* Convergence is linear ($r = 1, C = 0.5$) which is slow\n",
    "  * One bit of accuracy gained for each iteration (divide by half each iteration)\n",
    "* Not suitable for solving systems of nonlinear equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the root of the scalar function $x^2 - 4 \\sin(x) = 0$ using bisection method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root:  1.9337537628270212\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def bisection1d(fx, a, b, eps=np.finfo('d').eps):\n",
    "    \"\"\"\n",
    "    Compute the root of fx over some interval [a, b] using bisection.\n",
    "    \"\"\"\n",
    "    while abs(b - a) > eps:\n",
    "        m = a + (b - a) / 2.\n",
    "        if np.sign(fx(a)) != np.sign(fx(m)):\n",
    "            b = m\n",
    "        else:\n",
    "            a = m\n",
    "    return a\n",
    "\n",
    "\n",
    "def fx(x):\n",
    "    return x*x - 4. * math.sin(x)\n",
    "\n",
    "\n",
    "root = bisection1d(fx, a=1., b=3.)\n",
    "print(\"root: \", root)\n",
    "\n",
    "# Compare the value at root to 0.\n",
    "np.testing.assert_almost_equal(fx(root), 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05.05 Fixed-Point Iteration in 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fixed point** of a function $g$ is a value $x$ such that $x = g(x)$.\n",
    "* Iterative methods for nonlinear equations use fixed-point scheme:\n",
    "$$\n",
    "x_{k+1} = g(x_k)\n",
    "$$\n",
    "* For some nonlinear equation $f(x) = 0$ there can be **multiple equivalent** choices for $g$.\n",
    "\n",
    "Examples, let $f(x) = x^2 - x - 2$ so that $f(2) = 0$\n",
    "1. $g(x) = x^2 - 2$ and $g(2) = 2$\n",
    "2. $g(x) = 1 + 2/x$ and $g(2) = 2$\n",
    "3. $g(x) = \\sqrt{x + 2}$ and $g(2) = 2$\n",
    "\n",
    "#### Convergence\n",
    "If $x^* = g(x^*)$ and $|g'(x^*)| \\lt 1$ then there is an interval containing $x^*$ that converges using fixed-point iteration.\n",
    "\n",
    "If $|g'(x^*)| \\gt 1$, then fixed-point iteration diverges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05.06 Newton's Method in 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncated Taylor Series\n",
    "The function $f(x + h)$ is a linear function of $h$ that approximates $f$ near $x$.\n",
    "$$\n",
    "f(x + h) \\approx f(x) + f'(x) h\n",
    "$$\n",
    "\n",
    "Solve nonlinear equation by replacing with linear approximation using Taylor series.\n",
    "\n",
    "#### Newton's Method\n",
    "1. Start with some initial guess $x_0$.\n",
    "2. Compute $x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}$.\n",
    "3. Repeat the previous step until some stopping criteria is reached.\n",
    "\n",
    "\n",
    "**Convergence**\n",
    "\n",
    "Transform $f(x) = 0$ to the fixed point problem $g(x) = x - f(x) / f'(x)$.\n",
    "* Convergence rate is quadratic $(r = 2)$.\n",
    "* The initial guess, $x_0$, must be close to $f(x) = 0$ in order to converge.\n",
    "* Unlike bisection, Newton's method is **not** guaranteed to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the root of scalar function $x^2 - 4 \\sin(x) = 0$ using Newton's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root:  1.9337537628270216\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def newton1d(fx, dfx, x0, eps=np.finfo('d').eps):\n",
    "    \"\"\"\n",
    "    Compute the root of fx near x0 using Newton's method.\n",
    "    \"\"\"\n",
    "    xk, fxk = x0, fx(x0)\n",
    "    hk = 1.  # Any nonzero value.\n",
    "\n",
    "    # Stop iteration when hk = 0 or fxk = 0.\n",
    "    while abs(hk)/abs(xk) > eps or abs(fxk) > eps:\n",
    "        hk = -1. * fxk / dfx(xk)\n",
    "        xk = xk + hk\n",
    "        fxk = fx(xk)\n",
    "    return xk\n",
    "\n",
    "\n",
    "def fx(x):\n",
    "    return x*x - 4. * math.sin(x)\n",
    "\n",
    "def dfx(x):\n",
    "    return 2.*x - 4. * math.cos(x)\n",
    "\n",
    "\n",
    "root = newton1d(fx, dfx, x0=3., eps=1e-7)\n",
    "print(\"root: \", root)\n",
    "\n",
    "# Compare the value at root to 0.\n",
    "np.testing.assert_almost_equal(fx(root), 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05.07 Interpolating Methods in 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Secant Method\n",
    "Like Newton's Method, but replace explicit derivative with finite difference.\n",
    "$$\n",
    "x_{k+1} = x_k - f(x_k) \\frac{x_k - x_{k-1}}{f(x_k) - f(x_{k-1})}\n",
    "$$\n",
    "\n",
    "Convergence rate is superlinear ($r \\approx 1.618$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the root of scalar function $x^2 - 4 \\sin(x) = 0$ using Secant method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root:  1.9337537628270214\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def secant1d(fx, x0, eps=np.finfo('d').eps):\n",
    "    \"\"\"\n",
    "    Compute the root of fx near x0 using Secant method.\n",
    "    \"\"\"\n",
    "    # Initialize xk, xk1 and fxk, fxk1 to current & previous iterate.\n",
    "    xk, xk1 = x0, 2.*x0\n",
    "    fxk, fxk1 = fx(xk), fx(xk1)\n",
    "    hk = 1.  # Any nonzero value.\n",
    "\n",
    "    # Stop iteration when hk = 0 or fxk = 0.\n",
    "    while abs(hk)/abs(xk) > eps or abs(fxk) > eps:\n",
    "        hk = -1. * fxk * (xk - xk1)/(fxk - fxk1)\n",
    "        xk, xk1 = xk + hk, xk\n",
    "        fxk, fxk1 = fx(xk), fxk\n",
    "    return xk\n",
    "\n",
    "\n",
    "def fx(x):\n",
    "    return x*x - 4. * math.sin(x)\n",
    "\n",
    "root = secant1d(fx, x0=3., eps=1e-7)\n",
    "print(\"root: \", root)\n",
    "\n",
    "# Compare the value at root to 0.\n",
    "np.testing.assert_almost_equal(fx(root), 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Higher-Degree Interpolation\n",
    "Like Secant method, but replace 2-point interpolation with higher degree polyonmial.\n",
    "\n",
    "Quadratic interpolation aka Muller's method has superlinear convergence ($r \\approx 1.839$)\n",
    "\n",
    "#### Inverse Interpolation\n",
    "Fit $x_k$ as a function of $y_k = f(x_k)$ using polynomial $p(y)$.\n",
    "\n",
    "1. Start with initial $a, b, c$ with $b$ straddling $a$ and $c$.\n",
    "2. Compute\n",
    "$$\n",
    "u = f_b/f_c, \\quad v = f_b/f_a, \\quad w = f_a/f_c\n",
    "$$\n",
    "3. Compute\n",
    "$$\n",
    "p = v \\times (w \\times (u-w)(c-b) - (1-u)(b-a)) \\quad q = (w-1)(u-1)(v-1)\n",
    "$$\n",
    "4. Compute new solution $x_k = b + p / q$.\n",
    "5. Replace $c$ with old $a$, $a$ with old $b$, and $b$ with new $x_k$.\n",
    "6. Repeat from step 2.\n",
    "\n",
    "Inverse interpolation also has superlinear convergence ($r \\approx 1.839$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the root of scalar function $x^2 - 4 \\sin(x) = 0$ using Inverse Interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root:  1.9337537628270212\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def inverse_interp1d(fx, a, b, c, eps=np.finfo('d').eps):\n",
    "    \"\"\"\n",
    "    Compute the root of fx near x0 using Inverse interpolation.\n",
    "    \"\"\"\n",
    "    # Initialize fa, fb, fc.\n",
    "    fa, fb, fc = fx(a), fx(b), fx(c)\n",
    "    hk, xk = 1., 1.  # Any nonzero value.\n",
    "\n",
    "    # Stop iteration when hk = 0 or fb = 0.\n",
    "    while abs(hk)/abs(xk) > eps or abs(fb) > eps:\n",
    "        u, v, w = fb/fc, fb/fa, fa/fc\n",
    "        p = v * (w * (u-w) * (c-b) - (1.-u) * (b-a))\n",
    "        q = (w-1.) * (u-1.) * (v-1.)\n",
    "        hk = p/q\n",
    "        xk = b + hk\n",
    "        c, fc = a, fa\n",
    "        a, fa = b, fb\n",
    "        b, fb = xk, fx(xk)\n",
    "    return xk\n",
    "\n",
    "\n",
    "def fx(x):\n",
    "    return x*x - 4. * math.sin(x)\n",
    "\n",
    "root = inverse_interp1d(fx, a=1., b=2., c=3., eps=1e-7)\n",
    "print(\"root: \", root)\n",
    "\n",
    "# Compare the value at root to 0.\n",
    "np.testing.assert_almost_equal(fx(root), 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05.08 Hybrid Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secant and interpolation methods converge rapidly, but require initial values close to solution.\n",
    "\n",
    "Bisection method converges slowly, but are less sensitive to initial values.\n",
    "\n",
    "#### Hybrid methods\n",
    "Use secant or interpolation, but maintain a bracket around solution.\n",
    "* If an approximate solution ever falls outside of bracket, perform one iteration of bisection.\n",
    "* Combines features to achieve speed and reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05.09 Newton's Method for Nonlinear Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed-Point Iteration\n",
    "For fixed point methods of non-linear systems replace scalar $x$ with vector $x$.\n",
    "\n",
    "Problem statement: Find some vector $x$ such that $x = g(x)$ and $g: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$.\n",
    "\n",
    "#### Convergence\n",
    "If spectral radius $\\rho$ of Jacobian matrix $\\rho(G(x^*)) \\lt 1$ then fixed-point iteration converges if started close enough to solution.\n",
    "\n",
    "#### Newton's Method\n",
    "In n-dimensions Newton's method given by:\n",
    "$$\n",
    "x_{k+1} = x_k - J(x_k)^{-1} f(x_k)\n",
    "$$\n",
    "where\n",
    "* $J(x)$ is Jacobian matrix of $f$\n",
    "\n",
    "In practice, the Jacobian is not inverted but instead solve linear system for $s_k$:\n",
    "$$\n",
    "J(x_k) s_k = -f(x_k)\n",
    "$$\n",
    "where\n",
    "* $s_k$ is Newton step such that $x_{k+1} = x_k + s_k$\n",
    "\n",
    "Newton's method for vector functions has quadratic convergence ($r = 2$)\n",
    "\n",
    "Cost of Newton's method per iteration is substantial:\n",
    "* Computing Jacobian matrix: $n^2$\n",
    "* Solving linear system: $O(n^3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jacobian Matrix\n",
    "Given the nonlinear system of equations $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$:\n",
    "$$\n",
    "x_1 + 2 x_2 - 2 = 0 \\\\\n",
    "x_1^2 + 4 x_2^2 - 4 = 0\n",
    "$$\n",
    "\n",
    "The Jacobian matrix of $J(x)$ is formed from:\n",
    "$$\n",
    "{J_f(x)}_{ij} = \\frac{\\partial f_i(x)}{\\partial x_j}\n",
    "$$\n",
    "where\n",
    "* $f_i(x)$ is the ith system of equations\n",
    "\n",
    "Given the above system $f$, the partial derivatives are:\n",
    "$$\n",
    "\\frac{\\partial f_1(x)}{\\partial x_1} = \\frac{\\partial (x_1 + 2 x_2 - 2)}{\\partial x_1} = 1 \\\\\n",
    "\\frac{\\partial f_1(x)}{\\partial x_2} = \\frac{\\partial (x_1 + 2 x_2 - 2)}{\\partial x_2} = 2 \\\\\n",
    "\\frac{\\partial f_2(x)}{\\partial x_1} = \\frac{\\partial (x_1^2 + 4 x_2^2 - 4)}{\\partial x_1} = 2 x_1 \\\\\n",
    "\\frac{\\partial f_2(x)}{\\partial x_2} = \\frac{\\partial (x_1^2 + 4 x_2^2 - 4)}{\\partial x_2} = 8 x_2\n",
    "$$\n",
    "\n",
    "The Jacobian matrix formed from these partial derivatives is:\n",
    "$$\n",
    "J(x) =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f_1(x)}{\\partial x_1} & \\frac{\\partial f_1(x)}{\\partial x_2} \\\\\n",
    "\\frac{\\partial f_2(x)}{\\partial x_1} & \\frac{\\partial f_2(x)}{\\partial x_2}\n",
    "\\end{bmatrix} \\\\\n",
    "\\quad = \n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "2 x_1 & 8 x_2 \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Newton's method to solve the nonlinear system:\n",
    "$$\n",
    "x_1 + 2 x_2 - 2 = 0 \\\\\n",
    "x_1^2 + 4 x_2^2 - 4 = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root:  [-6.27144051e-09  1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "\n",
    "def newtonNd(fx, Jfx, x0, eps=np.finfo('d').eps):\n",
    "    \"\"\"\n",
    "    Solve the nonlinear n x n system using Newton's method.\n",
    "    \"\"\"\n",
    "    xk, fxk = x0, fx(x0)\n",
    "\n",
    "    # Stop iteration when ||f(xk)|| = 0.\n",
    "    while np.linalg.norm(fxk) > eps:\n",
    "        # Solve J(xk) sk = -f(sk) for sk.\n",
    "        sk = np.linalg.solve(Jfx(xk), -1. * fxk)\n",
    "        xk = xk + sk\n",
    "        fxk = fx(xk)\n",
    "    return xk\n",
    "\n",
    "\n",
    "def fx(xk):\n",
    "    \"\"\"\n",
    "    Compute the nonlinear system at xk.\n",
    "    \"\"\"\n",
    "    F = np.array([1,2,-2,1,4,-4], dtype='d').reshape(2,3)\n",
    "    x = np.stack((np.append(xk, 1), \n",
    "                  np.square(np.append(xk, 1))), axis=0)\n",
    "    return np.sum(F * x, axis=1)\n",
    "\n",
    "\n",
    "def Jfx(xk):\n",
    "    \"\"\"\n",
    "    Compute the Jacobian at xk.\n",
    "    \"\"\"\n",
    "    Jf = np.array([1,2,2,8], dtype='d').reshape(2,2)\n",
    "    x = np.stack(([1,1], xk), axis=0)\n",
    "    return Jf * x  # Elementwise multiplication.\n",
    "\n",
    "\n",
    "x0 = np.array([1,2])\n",
    "root = newtonNd(fx, Jfx, x0=x0, eps=1e-7)\n",
    "print(\"root: \", root)\n",
    "\n",
    "# Compare to the value returned from scipy.optimize.root.\n",
    "expected = opt.root(fx, x0, jac=Jfx)\n",
    "np.testing.assert_almost_equal(root, expected.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05.10 Secant Updating Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secant updating methods reduce cost of Newton's method by:\n",
    "* Build an approximate Jacobian rather than explicit derivatives\n",
    "* Use factorization of approximate Jacobian to avoid repeated computation at each iteration\n",
    "\n",
    "Secant updating methods have superlinear convergence ($1 \\lt r \\lt 2$)\n",
    "\n",
    "#### Broyden's Method\n",
    "1. Start with some initial guess $x_0$ and approximate Jacobian $B_0$.\n",
    "2. Solve $B_k s_k = -f(x_k)$ for $s_k$.\n",
    "3. Compute $x_{k+1} = x_k + s_k$.\n",
    "4. Compute difference in iterates $y_k = f(x_{k+1}) - f(x_k)$.\n",
    "5. Update approximate Jacobian (note: use of outer product in numerator)\n",
    "$$\n",
    "B_{k+1} = B_k + \\frac{(y_k - B_k s_k) s_k^T}{s_k^T s_k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Broyden's method to solve the nonlinear system:\n",
    "$$\n",
    "x_1 + 2 x_2 - 2 = 0 \\\\\n",
    "x_1^2 + 4 x_2^2 - 4 = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root:  [-7.78185824e-11  1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "\n",
    "def broyden(fx, B0, x0, eps=np.finfo('d').eps):\n",
    "    \"\"\"\n",
    "    Solve the nonlinear n x n system using Broyden's method.\n",
    "    \n",
    "    The value B0 is an initial guess for the approximate Jacobian.\n",
    "    \"\"\"\n",
    "    xk, fxk, Bk = x0, fx(x0), B0\n",
    "\n",
    "    # Stop iteration when ||f(xk)|| = 0.\n",
    "    while np.linalg.norm(fxk) > eps:\n",
    "        # Solve Bk sk = -f(xk) for sk.\n",
    "        sk = np.linalg.solve(Bk, -1. * fxk)\n",
    "        xk = xk + sk\n",
    "        fxk1 = fx(xk)\n",
    "        # Compute difference in iterates.\n",
    "        yk = fxk1 - fxk\n",
    "        # Update approximate Jacobian.\n",
    "        Bk = Bk + np.outer(yk - np.dot(Bk, sk), sk.T) / np.dot(sk.T, sk)\n",
    "        fxk = fxk1\n",
    "    return xk\n",
    "\n",
    "\n",
    "def fx(xk):\n",
    "    \"\"\"\n",
    "    Compute the nonlinear system at xk.\n",
    "    \"\"\"\n",
    "    F = np.array([1,2,-2,1,4,-4], dtype='d').reshape(2,3)\n",
    "    x = np.stack((np.append(xk, 1), \n",
    "                  np.square(np.append(xk, 1))), axis=0)\n",
    "    return np.sum(F * x, axis=1)\n",
    "\n",
    "\n",
    "# Initial guess and approximate Jacobian.\n",
    "x0 = np.array([1,2])\n",
    "B0 = np.array([1,2,2,16], dtype='d').reshape(2,2)\n",
    "root = broyden(fx, B0=B0, x0=x0, eps=1e-7)\n",
    "print(\"root: \", root)\n",
    "\n",
    "# Compare to the value returned from scipy.optimize.root.\n",
    "# NOTE(mmorais): When using `broyden1` explicit tolerance required.\n",
    "expected = opt.root(fx, x0, method='broyden1', tol=1e-7)\n",
    "np.testing.assert_almost_equal(root, expected.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Nonlinear Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bisection is a safe, but slow method suitable for 1D only.\n",
    "* Secant and Newton's method are fast, but risky methods.\n",
    "  * Newton's method in 1D requires explicit derivative and in 2D Jacobian.\n",
    "* Hybrid methods combine fast methods with bisection when approximate solution goes out of a bracket aka interval.\n",
    "* Newton's method in 2D is expensive since it requires computing Jacobian and solving a linear system.\n",
    "* Broyden's method (2D) reduces cost in comparison to Newton's method due to use of approximate Jacobian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
