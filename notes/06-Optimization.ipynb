{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06.01 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a function $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ and set $S \\subset \\mathbb{R}^n$ find $x^* \\in S$ such that $f(x^*) \\leq f(x)$ for all $x \\in S$.\n",
    "* Fully general, maximum of $f$ is the minimum of $-f$.\n",
    "\n",
    "$f$ is referred to as **objective function**\n",
    "* differentiable\n",
    "* linear or nonlinear\n",
    "\n",
    "$S$ is referred to as **constraint**\n",
    "* system of equations or inequalities\n",
    "* linear or nonlinear\n",
    "* if $S = \\mathbb{R}^n$ then problem is **unconstrained**\n",
    "\n",
    "#### Example Problem\n",
    "Minimize the surface area of cylinder subject to constraint on volume.\n",
    "$$\n",
    "\\min_{x_1, x_2} f(x_1, x_2) = 2 \\pi x_1 (x_1 + x_2) \\\\\n",
    "\\text{subject to} \\quad g(x_1, x_2) = \\pi x_1^2 x_2 - V = 0 \\\\\n",
    "$$\n",
    "where\n",
    "* $x_1, x_2$ are the radius and height of cylinder\n",
    "* $V$ is volume of cylinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06.02 Existence, Uniqueness, and Conditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Existence\n",
    "If $f$ is continuous on *closed* and *bounded* set $S \\in \\mathbb{R}^n$ then $f$ has global minimum on $S$.\n",
    "\n",
    "If $f$ is **coercive** (defined below) on *closed* and *unbounded* set $S \\in \\mathbb{R}^n$ then $f$ has global minimum on $S$.\n",
    "$$\n",
    "\\lim_{||x|| \\rightarrow \\infty} f(x) = + \\infty\n",
    "$$\n",
    "\n",
    "Examples\n",
    "* $f(x) = x^2$ is coercive, since set is unbounded and goes to $+ \\infty$ from both directions\n",
    "* $f(x) = x^3$ is not coercive, since it goes to $+ \\infty$ and $- \\infty$ depending on the sign of $x$\n",
    "\n",
    "No other fully general statements can be made about existence of minimum.\n",
    "\n",
    "#### Uniqueness\n",
    "Set $S \\in \\mathbb{R}^n$ is **convex** if the set **fully** contains a line segment between any two points in the set.\n",
    "\n",
    "Function $f: S \\in \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is **convex** if its graph along any line segment in $S$ lies on or below the chord connecting function values at endpoints of segment.\n",
    "\n",
    "Any local minimum of convex function $f$ on convex set $S \\in \\mathbb{R}^n$ is global minimum of $f$ on $S$.\n",
    "* The global minimum is unique when $f$ is strictly convex.\n",
    "\n",
    "#### Conditioning\n",
    "In comparison to solving nonlinear equations, minima can only be computed to half precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06.03 Optimality Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First-Order\n",
    "Find the minimum by solving nonlinear system with $\\nabla f(x) = 0$ where ith component of the gradient given by $\\partial f(x) / \\partial x_i$.\n",
    "* Values of $x$ where $\\nabla f(x) = 0$ referred to as **critical points**.\n",
    "* Critical points can be:\n",
    "  * minimum\n",
    "  * maximum\n",
    "  * saddle points\n",
    "\n",
    "#### Second-Order\n",
    "Distinguish critical points by using symmetric **Hessian matrix** $H_f (x)$ defined as:\n",
    "$$\n",
    "{H_f (x)}_{ij} = \\frac{\\partial^2 f(x)}{\\partial x_i \\partial x_j}\n",
    "$$\n",
    "* Hessian (second order partial derivative) related to Jacobian (first order partial derivatives) by: \n",
    "$$\n",
    "H(f(x)) = J(\\nabla f(x))^T\n",
    "$$\n",
    "\n",
    "At critical point $x^*$ if $H(f(x^*))$ is:\n",
    "* positive definite, then $x^*$ is minimum of $f$\n",
    "* negative definite, then $x^*$ is maximum of $f$\n",
    "* indefinite, then $x^*$ is saddle point of $f$\n",
    "* singular, then various outcomes are possible\n",
    "\n",
    "#### Constrained Optimality\n",
    "Lagrangian function\n",
    "$$\n",
    "\\mathcal{L}(x, \\lambda) = f(x) + \\lambda^T g(x)\n",
    "$$\n",
    "\n",
    "Gradient given by\n",
    "$$\n",
    "\\mathcal{L}(x, \\lambda) =\n",
    "\\begin{bmatrix}\n",
    "\\nabla f(x) + J_g^T (x) \\lambda \\\\\n",
    "g(x) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Hessian (symmetric) given by\n",
    "$$\n",
    "H_{\\mathcal{L}}(x, \\lambda) =\n",
    "\\begin{bmatrix}\n",
    "B(x, \\lambda) & J_g^T(x) \\\\\n",
    "J_g (x) & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "B(x, \\lambda) = H_f(x) + \\sum_{i=1}^{m} \\lambda_i H_{g_i} (x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06.04 One-Dimensional Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unimodal\n",
    "Real-valued function $f$ is **unimodal** on interval $[a, b]$ if there is a unique $x^* \\in [a, b]$ such that:\n",
    "* $f(x^*)$ is minimum of $f$ on $[a, b]$\n",
    "* $f$ is strictly decreasing for $x \\leq x^*$\n",
    "* $f$ is strictly increasing for $x^* \\leq x$\n",
    "\n",
    "For optimization problems, unimodality enables discarding portion of interval analogous to interval bisection for nonlinear equations.\n",
    "\n",
    "#### Golden Section Search\n",
    "Let $x_1$ and $x_2$ be two points on the interval $[a, b]$ and assume that $f$ is unimodal.\n",
    "\n",
    "* If $f(x_1) < f(x_2)$, then the minimum is not in interval $[x_2, b]$.\n",
    "* If $f(x_1) > f(x_2)$, then the minimum is not in interval $[a, x_1]$.\n",
    "\n",
    "How to choose the points $x_1$ and $x_2$?\n",
    "* Choose positions based on $\\tau$ and $1 - \\tau$ where $\\tau^2 = 1 - \\tau$ or $\\tau \\approx 0.618$ and $1 - \\tau \\approx 0.382$.\n",
    "* Positions $\\tau$ and $1 - \\tau$ known as **golden section search**.\n",
    "\n",
    "Convergence is linear ($r = 1, C \\approx 0.618$) which is slow.\n",
    "\n",
    "#### Parabolic Interpolation\n",
    "Fit quadratic polynomial to three function values.\n",
    "* Analogous to secant method, but with higher order polynomial instead of line.\n",
    "\n",
    "Convergence is superlinear ($r \\approx 1.324$).\n",
    "\n",
    "#### Newton's Method\n",
    "Approximate the function using Taylor series:\n",
    "$$\n",
    "f(x + h) \\approx f(x) + f'(x) h + \\frac{f''(x)}{2} h^2\n",
    "$$\n",
    "\n",
    "Differentiating by $h$, the minimum is given by:\n",
    "$$\n",
    "h = - \\frac{f'(x)}{f''(x)}\n",
    "$$\n",
    "\n",
    "Suggests the following method:\n",
    "1. Start with some initial guess $x_0$.\n",
    "2. Compute $x_{k+1} = x_k - \\frac{f'(x)}{f''(x)}$.\n",
    "3. Repeat the previous step until some stopping criteria is reached.\n",
    "\n",
    "\n",
    "Convergence rate is quadratic ($r = 2$).\n",
    "* The initial guess, $x_0$ must be close to minimum in order to converge.\n",
    "\n",
    "#### Safeguarded Methods\n",
    "As with nonlinear equations, slow-but-sure methods can be combined with fast-but-risky.\n",
    "* Popular combination for library routines is golden section search with parabolic interpolation.\n",
    "* Newton's Method requires derivatives and, as a result is not as popular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the minimum of $f(x) = 0.5 - x e^{-x^2}$ over the interval $[0, 2]$ using golden section search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minx:   0.7071067871818779\n",
      "minfx:  0.07111805751964656\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "\n",
    "def goldensection(fx, a, b, tol=np.finfo('d').eps):\n",
    "    \"\"\"\n",
    "    Find the minimum of f on interval [a, b] using Golden Section Search.\n",
    "    \"\"\"\n",
    "    tau = (math.sqrt(5) - 1.) / 2.\n",
    "    x1, x2 = a + (1. - tau) * (b - a), a + tau * (b - a)\n",
    "    fx1, fx2 = fx(x1), fx(x2)\n",
    "    \n",
    "    while (b - a) > tol:\n",
    "        if fx1 < fx2:\n",
    "            b = x2\n",
    "            # Treat x1 as the new x2.\n",
    "            x2, fx2 = x1, fx1\n",
    "            # Compute new x1.\n",
    "            x1 = a + (1. - tau) * (b - a)\n",
    "            fx1 = fx(x1)\n",
    "        else:\n",
    "            a = x1\n",
    "            # Treat x2 as the new x1.\n",
    "            x1, fx1 = x2, fx2\n",
    "            # Compute new x2.\n",
    "            x2 = a + tau * (b - a)\n",
    "            fx2 = fx(x2)\n",
    "    \n",
    "    return x1, fx1\n",
    "\n",
    "\n",
    "def fx(x):\n",
    "    return 0.5 - x * math.exp(-1. * x*x)\n",
    "\n",
    "\n",
    "tol=np.finfo('d').eps\n",
    "\n",
    "a, b = 0., 2.\n",
    "minx, minfx = goldensection(fx, a, b, tol=tol)\n",
    "print(\"minx:  \", minx)\n",
    "print(\"minfx: \", minfx)\n",
    "\n",
    "# Compare to the value returned from scipy.optimize.minimize_scalar.\n",
    "res = opt.minimize_scalar(fx, (a, b), method='golden', tol=tol)\n",
    "np.testing.assert_almost_equal(minx, res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the minimum of $f(x) = 0.5 - x e^{-x^2}$ over the interval $[0, 2]$ using Newton's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minx:   0.7071067811865475\n",
      "minfx:  0.07111805751964656\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "\n",
    "def newton1d(fx, dfx, ddfx, a, b, tol=np.finfo('d').eps):\n",
    "    \"\"\"\n",
    "    Find the minimum of f on interval [a, b] using Newton's method.\n",
    "    \"\"\"\n",
    "    # NOTE(mmorais): Newton's method very sensitive to x0.\n",
    "    # Initialized x0 to random value in [a, b], but the solution\n",
    "    # will frequently fail to converge.  Eventually settled on \n",
    "    # midpoint of the interval which converges for this problem.\n",
    "    #x0 = a + np.random.random() * (b - a)\n",
    "    x0 = a + (b - a) / 2.\n",
    "    xk, fxk = x0, fx(x0)\n",
    "    fxk_prev = fxk + 1.\n",
    "\n",
    "    # Stop iteration when there is no change in fxk.\n",
    "    while abs(fxk - fxk_prev) > tol:\n",
    "        hk = -1. * dfx(xk) / ddfx(xk)\n",
    "        xk = xk + hk\n",
    "        fxk_prev = fxk\n",
    "        fxk = fx(xk)\n",
    "\n",
    "    return xk, fxk\n",
    "\n",
    "\n",
    "def fx(x):\n",
    "    return 0.5 - x * math.exp(-1. * x*x)\n",
    "\n",
    "def dfx(x):\n",
    "    ex2 = math.exp(-1. * x*x)\n",
    "    return 2. * x*x * ex2 - ex2\n",
    "\n",
    "def ddfx(x):\n",
    "    ex2 = math.exp(-1. * x*x)\n",
    "    return 6. * x * ex2 - 4. * math.pow(x, 3) * ex2\n",
    "\n",
    "\n",
    "tol=np.finfo('d').eps\n",
    "\n",
    "a, b = 0., 2.\n",
    "minx, minfx = newton1d(fx, dfx, ddfx, a, b, tol=tol)\n",
    "print(\"minx:  \", minx)\n",
    "print(\"minfx: \", minfx)\n",
    "\n",
    "# Compare to the value returned from scipy.optimize.minimize_scalar.\n",
    "res = opt.minimize_scalar(fx, (a, b), tol=tol)\n",
    "np.testing.assert_almost_equal(minx, res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06.05 Unconstrained Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06.06 Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06.07 Quasi-Netwon Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06.08 Nonlinear Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06.09 Constrained Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
